{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa7ea71",
   "metadata": {},
   "source": [
    "# Blazakis solar irradiation forecast  <a class=\"tocSkip\">\n",
    "\n",
    "/ HYSTORY / -----------------------------------------------------------------\n",
    "\n",
    "2023/11/10: v0 (from `wind_v2.ipynb`) /\n",
    "\n",
    "2023/11/17: v0_1 /\n",
    "\n",
    "- Updated: Edited following `wind_v3_1`\n",
    "- Updated: Added `plot_a_few_windows`\n",
    "- Updated: Added single-window predictions\n",
    "- Updated: Added min/max filled bands in flattened predictions    \n",
    "- Fixed: In XGBoost, added `eval_set` to use `early_stopping_rounds`\n",
    "\n",
    "2023/11/21: v0_2 /\n",
    "\n",
    "- Updated: Replaced XGB with LightGBM\n",
    "\n",
    "2023/11/21: v2 /\n",
    "\n",
    "- Updated: Set patience as global parameter\n",
    "- Updated: Moved all layers imports up\n",
    "- Updated: Added Encoder-Decoder LSTM, but not used\n",
    "- Updated: Added Wavenet\n",
    "- Updated: Added models by Blazakis\n",
    "- Updated: Switched to ROCV protocol\n",
    "\n",
    "/ TODO / --------------------------------------------------------------------\n",
    "\n",
    "- Select on NORMALIZED MAE/RMSE since the dynamic range changes with the fold?    \n",
    "    \n",
    "/ NOTES / -------------------------------------------------------------------\n",
    "    \n",
    "- Shall L-S go _after_ normalization?\n",
    "- L-S shall go _after_ smoothing\n",
    "- Flip loop to perform windowing only `n_iter` times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cec3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import ipynbname\n",
    "import os\n",
    "\n",
    "# Script name and folder -----------------------------------------------------\n",
    "script_path = str(Path().absolute())\n",
    "print('Current path:', script_path)\n",
    "\n",
    "#script_vers = script_name.split(\"_v\")[-1].strip(\".py\")\n",
    "#script_name = ipynbname.name()\n",
    "#print('Script version: %s' % script_vers)\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Results folder -------------------------------------------------------------\n",
    "path_results_main = script_path + \"/results\"\n",
    "# folder where the current run's results shall be nested in\n",
    "\n",
    "if not os.path.exists(path_results_main):\n",
    "    os.makedirs(path_results_main)\n",
    "    print('Folder \"%s\" created.' % os.path.basename(path_results_main))\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# keep it quiet, jeez!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = script_path + \"/../data\"\n",
    "\n",
    "# Tunable variables ----------------------------------------------------------\n",
    "debug = False\n",
    "\n",
    "# Data:\n",
    "path_to_data = path_data+'/solar_irradiation_database.csv'\n",
    "\n",
    "columns_predictors = ['time (HHMM)', 'DHI', 'NDD/5MINS']\n",
    "column_of_interest = \"GHI\"\n",
    "#-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad1095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d32c45",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d2c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Loading:\n",
    "df = pd.read_csv(path_to_data, compression=None,  \\\n",
    "                 header=0, index_col=False, sep=',', quotechar='\"')\n",
    "display(df.head(5))\n",
    "\n",
    "if debug:\n",
    "    df = df.iloc[:10000]\n",
    "\n",
    "print('Original data points number:', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd4599",
   "metadata": {},
   "source": [
    "## Converting time to seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0062ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hhmm_to_seconds(decimal_time):\n",
    "    try:\n",
    "        hours = int(decimal_time // 100)\n",
    "        minutes = int(decimal_time % 100)\n",
    "        seconds_decimal = (decimal_time - int(decimal_time)) * 60\n",
    "        \n",
    "        if 0 <= hours < 24 and 0 <= minutes < 60 and 0 <= seconds_decimal < 60:\n",
    "            total_seconds = hours * 3600 + minutes * 60 + seconds_decimal\n",
    "            return total_seconds\n",
    "        else:\n",
    "            raise ValueError(\"Invalid HHMM time format\")\n",
    "    except (ValueError, IndexError):\n",
    "        raise ValueError(\"Invalid HHMM time format\")\n",
    "\n",
    "# Filling missing HHMMs ------------------------------------------------------\n",
    "#unique_HHMMs = [hour * 100 + minute for hour in range(24) for minute in range(0, 60, 5)]\n",
    "unique_HHMMs = [hour * 100 + minute for hour in range(7, 17) for minute in range(0, 60, 5)]\n",
    "# all possible times from <.> to <.>, sampled every 5 minutes\n",
    "\n",
    "# Create a DataFrame for each day with all unique times and merge with original data:\n",
    "dfs = []\n",
    "for day in df['day'].unique():\n",
    "    df_temp = pd.DataFrame({'day': day, 'time (HHMM)': unique_HHMMs}).\\\n",
    "                sort_values(['time (HHMM)'])\n",
    "    df_merged = pd.merge(df_temp, df, on=['day', 'time (HHMM)'], how='left')\n",
    "    dfs.append(df_merged)\n",
    "\n",
    "# Combine all DataFrames:\n",
    "df = pd.concat(dfs).sort_values(['day', 'time (HHMM)']).\\\n",
    "                reset_index(drop=True)\n",
    "#-----------------------------------------------------------------------------\n",
    "        \n",
    "# Converting -----------------------------------------------------------------\n",
    "df['time_s'] = df['time (HHMM)'].apply(hhmm_to_seconds) + df['day']*86400\n",
    "# 1 day = 60*60*24 = 86400 sec\n",
    "\n",
    "# Setting origin to 0:\n",
    "df['time_s'] = df['time_s'] - np.min(df['time_s'])\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Cast datatype to save RAM --------------------------------------------------\n",
    "for col in df.select_dtypes(include='float64').columns:\n",
    "    df[col] = df[col].astype('float32')\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Sanity check ---------------------------------------------------------------\n",
    "# Calculate the differences between consecutive time values:\n",
    "df_delta = df['time_s'].diff().round(4)\n",
    "# NOTE: `round` is used to avoid precision issues\n",
    "\n",
    "# Check where differences are not equal to the first difference:\n",
    "not_equal_to_first = df_delta.loc[1:] != df_delta.iloc[1]\n",
    "# NOTE: Comparing against the second entry (indexed by '1') because the first\n",
    "#       is NaN by construction (no previous times to compare to)\n",
    "\n",
    "# Get the indices where differences are not equal to the first difference\n",
    "idxs_not_equal = not_equal_to_first.index[not_equal_to_first]\n",
    "\n",
    "if idxs_not_equal.empty:\n",
    "    print(\"Check:: all timesteps equally spaced.\")\n",
    "else:\n",
    "    print(\"Check:: timesteps not equal to the first, at indices:\", list(idxs_not_equal))\n",
    "    print(\"'--> Expected difference (1st timestep):\", df_delta.iloc[1])\n",
    "    print(\"'--> Actual differences:\")\n",
    "    display(df_delta.iloc[idxs_not_equal])\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Selecting columns for analysis ---------------------------------------------\n",
    "df_ = pd.DataFrame(df[columns_predictors + [column_of_interest]])\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f29aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_MAX_ = int(0.05*len(df))\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(14, 7), nrows=2, ncols=1)\n",
    "\n",
    "# Separators:\n",
    "current_time_s = 0\n",
    "while current_time_s < np.max(df['time_s'].values):\n",
    "    current_time_s += 86400*30 # 1 month in seconds\n",
    "    axes[0].axvline(x=current_time_s, ls='-', lw=2, c='lightgrey', label='month separator')\n",
    "\n",
    "axes[0].set_title('Whole series')\n",
    "axes[0].set_xlabel('time_s')\n",
    "axes[0].set_ylabel('value')\n",
    "axes[0].plot(df['time_s'], df[column_of_interest].values, c='C0')\n",
    "axes[0].plot(df['time_s'].iloc[:idx_MAX_], df[column_of_interest].iloc[:idx_MAX_].values, c='C1')\n",
    "\n",
    "axes[1].set_title('Zoom-in on 5% of data')\n",
    "axes[1].set_xlabel('time_s')\n",
    "axes[1].set_ylabel('value')\n",
    "axes[1].plot(df['time_s'].iloc[:idx_MAX_], df[column_of_interest].iloc[:idx_MAX_].values, c='C1')\n",
    "\n",
    "# Separators:\n",
    "current_time_s = 0\n",
    "while current_time_s < df['time_s'].values[idx_MAX_]:\n",
    "    current_time_s += 86400 # 1 day in seconds\n",
    "    axes[1].axvline(x=current_time_s, ls='--', lw=2, c='lightgrey', label='day separator')\n",
    "    if current_time_s % int(86400*7) == 0: # 1 week in seconds\n",
    "        axes[1].axvline(x=current_time_s, ls='--', lw=4, c='lightgrey', label='week separator')\n",
    "\n",
    "# Removing redundant legend labels and plotting legend:\n",
    "for ax in axes:\n",
    "    ax.legend()\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique_legend = {}\n",
    "    for h, l in zip(handles, labels):\n",
    "        if l not in unique_legend:\n",
    "            unique_legend[l] = h\n",
    "    ax.legend_.remove()\n",
    "    ax.legend(unique_legend.values(), unique_legend.keys())\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077bd8d8",
   "metadata": {},
   "source": [
    "# Lomb-Scargle Periodgram - Testing\n",
    "\n",
    "This is in practice only needed if the data are sampled un-evenly in time, which is the case when e.g. there are gaps, such as in a CV situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from src.lomb_scargle_v0 import LS\n",
    "\n",
    "ls = LS(nterms=10)\n",
    "\n",
    "ls.fit(df, column_time='time_s', column_signal=column_of_interest)\n",
    "df_stat = ls.transform(df)\n",
    "df_recon = ls.inverse_transform(df_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f54d22",
   "metadata": {},
   "source": [
    "# Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf512521",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Extract \"time_s\" and \"signal\" columns:\n",
    "time_s = df.dropna()[\"time_s\"].values\n",
    "signal = df.dropna()[column_of_interest].values\n",
    "\n",
    "# Lags:\n",
    "n_lags = 500\n",
    "# number of lags to test (this number is guessed looking at the DFT results)\n",
    "\n",
    "# Calculate the time step (assuming a uniform time sampling) [sec]:\n",
    "time_step = time_s[1] - time_s[0]\n",
    "\n",
    "print('Testing %s lags, corresponding to a MAX of %.2f days' %\n",
    "     (n_lags, (time_step*n_lags)/86400))\n",
    "\n",
    "# Calculate autocorrelations:\n",
    "ACF = sm.tsa.acf(signal, nlags=n_lags)\n",
    "\n",
    "plot_acf(signal, lags=n_lags)\n",
    "plt.title(\"Autocorrelation (ACF) of %s\" % column_of_interest)\n",
    "plt.show()\n",
    "\n",
    "# Calculate partial autocorrelations:\n",
    "PACF = sm.tsa.pacf(signal, nlags=n_lags)\n",
    "\n",
    "plot_pacf(signal, lags=n_lags)\n",
    "plt.title(\"Partial Autocorrelation (PACF) of %s\" % column_of_interest)\n",
    "plt.show()\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf62f7d",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c0365",
   "metadata": {},
   "source": [
    "## Constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cd9513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Conv1D, MaxPooling1D, Flatten,\\\n",
    "                         BatchNormalization, Activation, Dropout, Input,\\\n",
    "                         RepeatVector, Add, Multiply, concatenate, Layer\n",
    "#\n",
    "from tensorflow.keras import regularizers\n",
    "#\n",
    "import time\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "        \n",
    "time_callback = TimeHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298cecb5",
   "metadata": {},
   "source": [
    "### Multi-Channel CNN constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd553a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MCCNN_model(win_size=24, n_targets=24, n_features=3, batch_size=32,\n",
    "                    lr=1e-3):\n",
    "    '''NOTE: Not all input parameters are used in the constructor, but appear\n",
    "    in the function definition so that the whole config dictionary can be\n",
    "    passed to it.'''\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(Input(shape=(win_size, n_features)))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=16, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_targets, activation='relu'))\n",
    "    model.add(Dense(n_targets, activation='linear'))\n",
    "\n",
    "    opt = Adam(learning_rate=lr) \n",
    "\n",
    "    model.compile(loss=root_mean_squared_error, optimizer=opt, \n",
    "                  metrics=['MAE', root_mean_squared_error])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_MCCNN_model()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9439b2df",
   "metadata": {},
   "source": [
    "### Distributed LSTM constructor\n",
    "(Blazakis' GHI-LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c142ba6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_DLSTM_model(win_size=24, n_targets=24, n_features=3, batch_size=32,\n",
    "                     lr=1e-3):\n",
    "    '''NOTE: Not all input parameters are used in the constructor, but appear\n",
    "    in the function definition so that the whole config dictionary can be\n",
    "    passed to it.'''\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    reg = regularizers.l2(5e-3)\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=( win_size, n_features)))\n",
    "    model.add(keras.layers.LSTM(32, activation='tanh', return_sequences=True,\n",
    "                            kernel_regularizer=reg,\n",
    "                            recurrent_regularizer=reg))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.LSTM(32, activation='tanh', return_sequences=True,\n",
    "                            kernel_regularizer=reg,\n",
    "                            recurrent_regularizer=reg))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.TimeDistributed(keras.layers.Dense(32, activation='relu', kernel_regularizer=reg)))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.TimeDistributed(keras.layers.Dense(1, activation='linear')))\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr) \n",
    "\n",
    "    model.compile(loss=root_mean_squared_error, optimizer=opt, \n",
    "                  metrics=['MAE', root_mean_squared_error])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_DLSTM_model()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1cd99",
   "metadata": {},
   "source": [
    "### Multi-Head CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0280f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSplitLayer(Layer):\n",
    "    def __init__(self, n_features, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return [tf.reshape(inputs[:, :, i], (-1, tf.shape(inputs)[1], 1)) for i in range(self.n_features)]\n",
    "\n",
    "\n",
    "def build_MHCNN_model(win_size=24, n_targets=24, n_features=3, batch_size=32,\n",
    "                     lr=1e-3):\n",
    "    '''NOTE: Not all input parameters are used in the constructor, but appear\n",
    "    in the function definition so that the whole config dictionary can be\n",
    "    passed to it.'''\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    in_layer = Input(shape=(win_size, n_features))\n",
    "   \n",
    "    # Split the input into n_features parts\n",
    "    split_layers = FeatureSplitLayer(n_features)(in_layer)\n",
    "\n",
    "    out_layers = []\n",
    "    for split_input in split_layers:\n",
    "        conv1 = Conv1D(32, 3, activation='relu')(split_input)\n",
    "        conv2 = Conv1D(32, 3, activation='relu')(conv1)\n",
    "        pool1 = MaxPooling1D()(conv2)\n",
    "        flat = Flatten()(pool1)\n",
    "        out_layers.append(flat)\n",
    "    \n",
    "    merged = concatenate(out_layers)\n",
    "\n",
    "    dense1 = Dense(200, activation='relu')(merged)\n",
    "    dense2 = Dense(100, activation='tanh')(dense1)\n",
    "    outputs = Dense(n_targets)(dense2)\n",
    "\n",
    "    model = Model(inputs=in_layer, outputs=outputs)    \n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr) \n",
    "\n",
    "    model.compile(loss=root_mean_squared_error, optimizer=opt, \n",
    "                  metrics=['MAE', root_mean_squared_error])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_MHCNN_model()\n",
    "print(model.summary())\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True) #, to_file='model.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ab64c",
   "metadata": {},
   "source": [
    "### LSTM constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeacea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM_model(win_size=24, n_targets=24, n_features=3, batch_size=32,\n",
    "                     lr=1e-3):\n",
    "    '''NOTE: Not all input parameters are used in the constructor, but appear\n",
    "    in the function definition so that the whole config dictionary can be\n",
    "    passed to it.'''\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    reg = regularizers.l2(5e-3)\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=( win_size, n_features)))\n",
    "    model.add(keras.layers.LSTM(32,  activation='tanh', return_sequences=True,\n",
    "                            kernel_regularizer=reg,\n",
    "                            recurrent_regularizer=reg))\n",
    "    model.add(keras.layers.LSTM(32,  activation='tanh',\n",
    "                            kernel_regularizer=reg,\n",
    "                            recurrent_regularizer=reg))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dense(n_targets, activation='linear'))\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr) \n",
    "\n",
    "    model.compile(loss=root_mean_squared_error, optimizer=opt, \n",
    "                  metrics=['MAE', root_mean_squared_error])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_LSTM_model()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf0022",
   "metadata": {},
   "source": [
    "### WaveNet constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf1c412",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def WaveNetResidualConv1D(n_filters, kernel_size, dilation_rate, input_n_filters):\n",
    "    '''Function for creating a residual block in the WaveNet structure.'''\n",
    "    \n",
    "    def build_residual_block(l_input):\n",
    "        # Gated activation\n",
    "        l_sigmoid_conv1d = Conv1D(n_filters, kernel_size, dilation_rate=dilation_rate, \n",
    "                                  padding='same', activation='sigmoid')(l_input)\n",
    "        l_tanh_conv1d = Conv1D(n_filters, kernel_size, dilation_rate=dilation_rate, \n",
    "                               padding='same', activation='tanh')(l_input)\n",
    "        l_mul = Multiply()([l_sigmoid_conv1d, l_tanh_conv1d])\n",
    "\n",
    "        # Skip connection:\n",
    "        l_skip_connection = Conv1D(n_filters, 1)(l_mul)\n",
    "\n",
    "        # Transforming input to have the same number of filters before adding:\n",
    "        if n_filters != input_n_filters:\n",
    "            l_input_transformed = Conv1D(n_filters, 1)(l_input)\n",
    "        else:\n",
    "            l_input_transformed = l_input\n",
    "\n",
    "        l_residual = Add()([l_input_transformed, l_skip_connection])\n",
    "\n",
    "        return l_residual, l_skip_connection\n",
    "\n",
    "    return build_residual_block\n",
    "\n",
    "def build_WaveNet_model(win_size=24, n_targets=24, n_features=3, n_filters=32,\n",
    "                        kernel_size=2, n_residual_blocks=2,\n",
    "                        batch_size=32, lr=1e-3):\n",
    "    \n",
    "    # Converting any float to int, in case the parameter is passed as float:\n",
    "    n_residual_blocks = int(n_residual_blocks)\n",
    "    n_filters         = int(n_filters)\n",
    "    kernel_size       = int(kernel_size)\n",
    "    \n",
    "    inputs = Input(shape=(win_size, n_features))\n",
    "    x = inputs\n",
    "\n",
    "    skip_connections = []\n",
    "\n",
    "    # Stacked residual blocks:\n",
    "    for i in range(n_residual_blocks):\n",
    "        x, skip = WaveNetResidualConv1D(n_filters, kernel_size, 2**i, n_features)(x)\n",
    "        skip_connections.append(skip)\n",
    "\n",
    "    # Add all skip connections:\n",
    "    net = Add()(skip_connections)\n",
    "    net = Activation('relu')(net)\n",
    "    net = Dropout(0.2)(net)\n",
    "    net = Conv1D(n_filters, kernel_size=1, activation='relu')(net)\n",
    "    net = Dropout(0.2)(net)\n",
    "    net = Flatten()(net)\n",
    "    net = Dense(n_targets)(net)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=net)\n",
    "\n",
    "    opt = Adam(learning_rate=lr) \n",
    "    \n",
    "    model.compile(loss=root_mean_squared_error, optimizer=opt, \n",
    "                  metrics=['MAE', root_mean_squared_error])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_WaveNet_model()\n",
    "model.summary()\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True) #, to_file='model.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a3af6e",
   "metadata": {},
   "source": [
    "### Encoder-Decoder LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ED_LSTM_model:\n",
    "    def __init__(self, win_size=24, n_targets=24, n_features=3, n_decoder_features=1,\n",
    "                 n_units=64, lr=1e-3, batch_size=32):\n",
    "        '''NOTE: Not all input parameters are used in the constructor, but appear\n",
    "        in the function definition so that the whole config dictionary can be\n",
    "        passed to it.'''\n",
    "        \n",
    "        self.n_features = n_features\n",
    "        self.n_decoder_features = n_decoder_features\n",
    "        self.win_size   = win_size\n",
    "        self.n_targets  = n_targets\n",
    "        self.n_units    = n_units\n",
    "        self.lr         = lr\n",
    "        self.built = False\n",
    "        \n",
    "        # Build the model\n",
    "        self._build()\n",
    "        self.built = True\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        # Encoder\n",
    "        encoder_inputs = Input(shape=(self.win_size, self.n_features))\n",
    "        encoder = LSTM(self.n_units, return_state=True)\n",
    "        encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "        encoder_states = [state_h, state_c]\n",
    "\n",
    "        # Decoder\n",
    "        decoder_inputs = Input(shape=(self.n_targets, self.n_decoder_features))\n",
    "        decoder_lstm = LSTM(self.n_units, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "        decoder_dense = Dense(self.n_decoder_features)\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        # Model\n",
    "        self.ed_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "        \n",
    "        # Compile:\n",
    "        opt = Adam(learning_rate=self.lr)\n",
    "        self.ed_model.compile(loss=root_mean_squared_error, optimizer=opt, \n",
    "                      metrics=['MAE', root_mean_squared_error])\n",
    "\n",
    "        # Inference Encoder\n",
    "        self.encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "        # Inference Decoder\n",
    "        decoder_state_input_h = Input(shape=(self.n_units,))\n",
    "        decoder_state_input_c = Input(shape=(self.n_units,))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "            decoder_inputs, initial_state=decoder_states_inputs\n",
    "        )\n",
    "        decoder_states = [state_h, state_c]\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        self.decoder_model = Model(\n",
    "            [decoder_inputs] + decoder_states_inputs,\n",
    "            [decoder_outputs] + decoder_states\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit(self, X_y_shift, y, batch_size=None, epochs=1, shuffle=True,\n",
    "                        callbacks=None, validation_data=None,):\n",
    "        '''\n",
    "        Fit the full encoder-decoder model.\n",
    "        \n",
    "        Using Teacher Forcing strategy, both for training and validation.\n",
    "        In this setup, the true targets (shifted by 1 temstep backwards) are\n",
    "        passed as an additional input to the decoder.\n",
    "        \n",
    "        NOTE: Ideally, the validation set should not use the Teacher Forcing.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_y_shift : list\n",
    "            Contains all the necessary inputs:\n",
    "                o `X`: the data matrix\n",
    "                o `y_shift`: the shifted target matrix\n",
    "            The data are passed in this fashion to be compatible with the\n",
    "            expected input to `ed_model.fit()`, determined by `_build_decoder`.\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        X       = X_y_shift[0]\n",
    "        y_shift = X_y_shift[1]\n",
    "        \n",
    "        self.history = self.ed_model.fit([X, y_shift], y, batch_size=batch_size, epochs=epochs,\n",
    "           shuffle=shuffle, callbacks=callbacks, validation_data=validation_data,)\n",
    "        \n",
    "        self.is_fitted_ = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''Predict one timestep at a time, then collating results.'''\n",
    "        # Get the initial states from the encoder:\n",
    "        states_value = self.encoder_model.predict(X, verbose=0)\n",
    "\n",
    "        # Prepare the initial input for the decoder [initialized to 0s]:\n",
    "        decoder_input = np.zeros((X.shape[0], self.n_targets, self.n_decoder_features))\n",
    "\n",
    "        # Prepare the initial output for the decoder [initialized to 0s]:\n",
    "        output_seq = np.zeros((X.shape[0], self.n_targets, self.n_decoder_features))\n",
    "\n",
    "        # > Collect predictions\n",
    "        for t in tqdm(range(self.n_targets), desc=\"Predicting\"):\n",
    "\n",
    "            output, h, c = self.decoder_model.predict([decoder_input] + states_value, verbose=0)\n",
    "\n",
    "            # Update the decoder input with the predicted output:\n",
    "            #decoder_input[:, t, :] = output[:, t, :]\n",
    "            decoder_input = output\n",
    "            \n",
    "            # Update states:\n",
    "            states_value = [h, c]\n",
    "            \n",
    "            # Store predictions:\n",
    "            output_seq[:, t, :] = output[:, -1, :]\n",
    "\n",
    "        # Removing redundant features in case output is 1D:\n",
    "        output_seq = output_seq.squeeze()\n",
    "        \n",
    "        return output_seq\n",
    "    \n",
    "    def summary(self):\n",
    "        return self.ed_model.summary()\n",
    "    \n",
    "model = ED_LSTM_model()\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model.ed_model, show_shapes=True) #, to_file='model.png'\n",
    "\n",
    "# > Testing predictor\n",
    "dummy_encoder_input = np.random.rand(100, 24, 3)\n",
    "dummy_decoder_input = np.random.rand(100, 24, 1)\n",
    "\n",
    "# Call the predict method with dummy data\n",
    "predicted_output = model.predict(dummy_encoder_input)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b9355",
   "metadata": {},
   "source": [
    "## Setting and visualizing configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8628b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def remove_irrelevant_param(row):\n",
    "    '''Set the parameters irrelevant to the input regressor to `NaN`.'''\n",
    "    regr = row['regr']\n",
    "    # regressor for input row\n",
    "    \n",
    "    for column_name in row.index:\n",
    "        if \"__\" in column_name and regr not in column_name:\n",
    "            row[column_name] = np.nan\n",
    "    return row\n",
    "\n",
    "def apply_exceptions(df):\n",
    "    '''Apply regressor-specific exceptions.'''\n",
    "\n",
    "    df_out = df\n",
    "    \n",
    "    df_out.loc[df['regr'].str.contains('ED_LSTM'), 'stride'] = 1\n",
    "    '''Force ED_LSTM configurations to have strides of 1, or else the \n",
    "    construction of shifted y (additional decode input) will not work.'''\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def generate_configurations(config_grid):\n",
    "    '''Function that loops through the available combinations inside a \n",
    "    configuration grid'''\n",
    "\n",
    "    # Removing all configs related to unused regressors:\n",
    "    config_grid = {key: value for key, value in config_grid.items() if\\\n",
    "                        any(\n",
    "                            key.startswith(regr+'__')\n",
    "                            for regr in config_grid['regr']\n",
    "                        )\n",
    "                       or '__' not in key\n",
    "                  }\n",
    "    \n",
    "    param_keys = list(config_grid.keys())\n",
    "\n",
    "    # Generate all possible combinations:\n",
    "    combinations = list(product(*config_grid.values()))\n",
    "    df_combinations = pd.DataFrame(combinations, columns=param_keys)\n",
    "\n",
    "    # Identify columns containing lists (necessary to `drop_duplicates`):\n",
    "    list_columns = [col for col in df_combinations.columns if\n",
    "            all(isinstance(item, list) for item in df_combinations[col])]\n",
    "    \n",
    "    # Converting lists to tuples (necessary to `drop_duplicates`)\n",
    "    df_combinations[list_columns] = df_combinations[list_columns].\\\n",
    "                                            applymap(tuple)\n",
    "    \n",
    "    # Set the parameters irrelevant to a regressor to `NaN`:\n",
    "    df_combinations_NaNs = df_combinations.copy(deep=True)\n",
    "    for r, row in df_combinations.iterrows():\n",
    "    # r = row index\n",
    "        df_combinations_NaNs.iloc[r] = remove_irrelevant_param(row)\n",
    "    \n",
    "    # Apply exceptions:\n",
    "    df_combinations_NaNs_ex = apply_exceptions(df_combinations_NaNs)\n",
    "    \n",
    "    # Drop duplicate rows:\n",
    "    df_combinations_unique = df_combinations_NaNs.drop_duplicates()\n",
    "\n",
    "    # Create a set to track seen configurations:\n",
    "    seen_configs = set()\n",
    "\n",
    "    for _, combination in df_combinations_unique.iterrows():\n",
    "        config = dict(zip(param_keys, combination))\n",
    "\n",
    "        # Convert list values to tuples for hashing and comparison\n",
    "        for key, val in config.items():\n",
    "            if isinstance(val, list):\n",
    "                config[key] = tuple(val)\n",
    "\n",
    "        # Check if the configuration has been seen before\n",
    "        config_tuple = tuple(sorted(config.items()))\n",
    "        if config_tuple not in seen_configs:\n",
    "            seen_configs.add(config_tuple)\n",
    "            yield config\n",
    "\n",
    "def sample_configs(configs, n):\n",
    "    '''Samples <n> configs at random, but stratify on classifier.'''\n",
    "    \n",
    "    df = pd.DataFrame(configs)\n",
    "    \n",
    "    # Sample one config for each regressors first:\n",
    "    df_one_per_regr = df.groupby('regr', group_keys=False).\\\n",
    "                            apply(lambda x: x.sample(1))\n",
    "\n",
    "    # Calculate how many samples we need after storing one for each regressor:\n",
    "    n_remaining = n - len(df_one_per_regr)\n",
    "\n",
    "    # Return if we have more or just enough regressors to fulfill <n>:\n",
    "    if n_remaining <= 0:\n",
    "        return df_one_per_regr.sample(n, random_state=42).to_dict('records')\n",
    "        # one last sampling, in case we have more than <n> unique regressors\n",
    "\n",
    "    # Exclude the configs we've already taken:\n",
    "    df_remaining = df.drop(df_one_per_regr.index)\n",
    "\n",
    "    # Randomly select the remaining samples:\n",
    "    df_additional = df_remaining.sample(min(n_remaining, len(df_remaining)),\\\n",
    "                                        replace=False)\n",
    "\n",
    "    # Combine the two sets of samples:\n",
    "    df_sampled = pd.concat([df_one_per_regr, df_additional])\n",
    "\n",
    "    return df_sampled.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f55dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_grid = {\n",
    "    'N_freqs'   : [0],\n",
    "    'nterms'    : [10],\n",
    "    'win_size'  : [120],\n",
    "    'n_targets' : [120],\n",
    "    'stride'    : [1],\n",
    "    'regr'      : ['MCCNN', 'DLSTM', 'MHCNN', 'LSTM', 'WaveNet', 'MLP', 'LR', 'LGB'],\n",
    "    'win_smooth': [1, 5, 10, 20],\n",
    "    'MCCNN__batch_size': [32],\n",
    "    'MCCNN__lr':         [1e-3, 0.005],\n",
    "    'DLSTM__batch_size': [32],\n",
    "    'DLSTM__lr':         [1e-3, 0.005],\n",
    "    'MHCNN__batch_size': [32],\n",
    "    'MHCNN__lr':         [1e-3, 0.005],\n",
    "    'WaveNet__n_residual_blocks': [1, 3],\n",
    "    'WaveNet__n_filters':         [16, 32],\n",
    "    'WaveNet__batch_size':        [32],\n",
    "    'WaveNet__lr':                [1e-3, 0.005],\n",
    "    'LSTM__batch_size': [32],\n",
    "    'LSTM__lr':         [1e-3, 0.005],\n",
    "    'LR__fit_intercept': [True],\n",
    "    'MLP__hidden_layer_sizes' : [[200, 200, 200, 200], [500, 500, 500]],\n",
    "    'LGB__boosting_type'    : ['gbdt'],\n",
    "    'LGB__num_leaves'       : [16, 32],\n",
    "    'LGB__reg_alpha'        : [0.1],\n",
    "    'LGB__n_estimators'     : [100, 200],\n",
    "    'LGB__max_depth'        : [6, 12],\n",
    "    'LGB__subsample'        : [0.8],\n",
    "    'LGB__colsample_bytree' : [0.8],\n",
    "    'LGB__learning_rate'    : [0.05, 0.1],\n",
    "}\n",
    "# `N_freqs`: int or 'auto' (set 0 to skip L-S modelling)\n",
    "# `regr`   : any subset of ['MCCNN', 'DLSTM', 'MHCNN', 'LSTM', 'WaveNet', 'MLP', 'LR', 'LGB'],\n",
    "\n",
    "n_iter = 50\n",
    "# number of configurations to try at random, within the grid\n",
    "\n",
    "# Creating config grid:\n",
    "configs = [config for config in generate_configurations(config_grid)]\n",
    "\n",
    "# Checking that there are no duplicate entries:\n",
    "unique_entries = {tuple(sorted(d.items())) for d in configs}\n",
    "configs = [dict(entry) for entry in unique_entries]\n",
    "\n",
    "# Pick <n_iter> configs at random, at least 1 per regressor:\n",
    "configs = sample_configs(configs, n_iter)\n",
    "\n",
    "# Printing configurations as a fail-safe check:\n",
    "print('_____ %s configurations to be explored _____' % len(configs))\n",
    "display(pd.DataFrame(configs))\n",
    "\n",
    "# Store configs:\n",
    "df_configs = pd.DataFrame(configs)\n",
    "df_configs.to_csv(path_results_main+'/configs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13896d67",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e76f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Custom sqrt Transformer ----------------------------------------------------\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class SquareRootTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.sqrt(X)\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        return X**2    \n",
    "#-----------------------------------------------------------------------------\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_a_few_windows(X_idxs, X, y_idxs, y, df_, title='Windowed data'):\n",
    "    '''Plots a few windows of a windowed dataset.'''\n",
    "    \n",
    "    n_features = X.shape[-1]\n",
    "    fig, axes = plt.subplots(figsize=(14, 5*n_features), nrows=2*n_features,\n",
    "                             ncols=2, constrained_layout=True)\n",
    "\n",
    "    plt.suptitle(title, fontsize=14)\n",
    "\n",
    "    axes[0, 0].set_title('A few predicting windows')\n",
    "    axes[0, 1].set_title('Corresponding target windows [repeated]')\n",
    "    axes[n_features, 0].set_title('Zoom-in on one window/target')\n",
    "\n",
    "    # Iterating over features:\n",
    "    for j in range(n_features):\n",
    "\n",
    "        # Top panels:\n",
    "        for m in range(5):\n",
    "            axes[j, 0].plot(X_idxs[m, :], X[m, :, j] - m/10,\n",
    "                            c='C0', alpha=(1-1/5*m))\n",
    "            axes[j, 1].scatter(y_idxs[m], y[m] - m/10, s=5,\n",
    "                            c='purple', alpha=(1-1/5*m))\n",
    "            axes[j, 1].plot(y_idxs[m], y[m] - m/10,\n",
    "                            c='purple', alpha=(1-1/5*m))\n",
    "\n",
    "        # Bottom panels:\n",
    "        axes[j+n_features, 0].plot(X_idxs[0, :], X[0, :, j], c='C0')\n",
    "        axes[j+n_features, 1].scatter(y_idxs[0], y[0], s=5, c='purple')\n",
    "        axes[j+n_features, 1].plot(y_idxs[0], y[0], c='purple')\n",
    "\n",
    "        axes[j, 0].set_ylabel(df_.columns[j])\n",
    "        axes[j+n_features, 0].set_ylabel(df_.columns[j])\n",
    "\n",
    "    for ax in axes[:, 0]: ax.sharex(axes[0, 0])\n",
    "    for ax in axes[:, 1]: ax.sharex(axes[0, 1])\n",
    "\n",
    "    for m in [0, 1]:\n",
    "        axes[m*n_features, 0].set_xlabel('index')\n",
    "        axes[m*n_features, 1].set_xlabel('index')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(hspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78593017",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train/Test splitting:\n",
    "from prettytable import PrettyTable\n",
    "# Stationary:\n",
    "from src.lomb_scargle_v0 import LS\n",
    "# Windowing:\n",
    "from src.windowing_multi_v0_1 import windowing_multi, invert_windowing\n",
    "# Cross-validating:\n",
    "from src.ROCV_v0 import ROCVFold\n",
    "import time\n",
    "# Imputing:\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Defining model:\n",
    "from src.CustomEstimator_v0_1_solar import CustomEstimator\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from lightgbm import LGBMRegressor\n",
    "# Scaling:\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "# Performance:\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "#from sklearn.metrics import mean_absolute_percentage_error   \n",
    "\n",
    "# Train/Test splitting -------------------------------------------------------\n",
    "# Hard split of the data\n",
    "\n",
    "trainval_ratio = 0.9\n",
    "# fraction of data to be used for CV\n",
    "\n",
    "idxs_trainval = np.arange(int(trainval_ratio*len(df_)))\n",
    "# CV data\n",
    "idxs_htest    = np.arange(int(trainval_ratio*len(df_)), len(df_))\n",
    "# hold-out test set\n",
    "\n",
    "df_trainval = df_.iloc[idxs_trainval]\n",
    "df_htest    = df_.iloc[idxs_htest]\n",
    "\n",
    "table = PrettyTable()\n",
    "table.title = str('Data split before CV')\n",
    "table.field_names = ['set', 'size']\n",
    "table.add_row(['Trainval', len(df_trainval)])\n",
    "table.add_row(['Hold-out', len(df_htest)])\n",
    "print(table)\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Setting CV -----------------------------------------------------------------\n",
    "rocvf = ROCVFold(n_splits=5, frac_used=0.75, frac_train=0.7, frac_valid=0.15, display=False)\n",
    "# Place <small>% of `validtest` into validation, the rest into test:\n",
    "'''NOTE: The validation is only used by the NNs, but it is nevertheless\n",
    "         created adn ignored even when running the other algorithms,\n",
    "         in order to guarantee a fair assessment comparison.\n",
    "'''\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "df_scores = pd.DataFrame()\n",
    "# dataframe or results indexed by config `label`\n",
    "\n",
    "for c, config in enumerate(configs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    ##### SETTING CONFIGURATION ##############################################\n",
    "\n",
    "    # Extracting configuration variables for easier readability:\n",
    "    N_freqs    = config['N_freqs']\n",
    "    nterms     = config['nterms']\n",
    "    win_size   = config['win_size']\n",
    "    n_targets  = config['n_targets']\n",
    "    stride     = config['stride']\n",
    "    win_smooth = config['win_smooth']\n",
    "    #\n",
    "    regr      = config['regr']\n",
    "    #\n",
    "    label = \"config_\"+str(c)\n",
    "    #\n",
    "    regr_params = {key.replace(regr+'__', ''): value for key, value\\\n",
    "                   in config.items() if key.startswith(regr+'__')}    \n",
    "    # dictionary of parameters for the regressor in this `config\n",
    "    # (the rest of parameters, i.e. for other regressor types, will be ignored)`\n",
    "    \n",
    "    # Setting additional parameters:\n",
    "    patience = 5\n",
    "    \n",
    "    print(' %s ' % ('='*103))\n",
    "    print('|%s CONFIGURATION ID: %-9s [%2s/%2s] %s|' %\\\n",
    "          ('='*33, label, c+1, len(configs), '='*33))\n",
    "    print(' %s ' % ('='*103))\n",
    "    display(pd.DataFrame.from_dict(\\\n",
    "        {key: str(value) for key, value in config.items()}, orient='index').T)\n",
    "    ##########################################################################\n",
    "   \n",
    "    ##### CV LOOP ############################################################\n",
    "    for i, (idxs_train, idxs_valid, idxs_test) in enumerate(rocvf.split(df_trainval)):\n",
    "\n",
    "        print('\\n|%s %-9s | Fold %2s %s|' % ('-'*33, label, i, '-'*33))\n",
    "        \n",
    "        # Splitting folds ----------------------------------------------------        \n",
    "        df_train = df_.iloc[idxs_train]\n",
    "        df_valid = df_.iloc[idxs_valid]\n",
    "        df_test  = df_.iloc[idxs_test]\n",
    "\n",
    "        df_train_full = df.iloc[idxs_train]\n",
    "        df_valid_full = df.iloc[idxs_valid]\n",
    "        df_test_full  = df.iloc[idxs_test]\n",
    "        #---------------------------------------------------------------------\n",
    "\n",
    "        # Displaying fold size -----------------------------------------------\n",
    "        fig, ax = plt.subplots(figsize=(12, 3), nrows=1, ncols=1,\n",
    "                                 gridspec_kw={'hspace': 0.5})\n",
    "\n",
    "        fig.suptitle(str('Location of validation fold: %s' % i), fontsize=12)\n",
    "        ax.plot(df['time_s'], df[column_of_interest].values,\\\n",
    "                   lw=1, c='lightgrey', label='full')\n",
    "        ax.plot(df_train_full['time_s'], df_train[column_of_interest].values,\\\n",
    "                   lw=1, c='C0', label='train')\n",
    "        ax.plot(df_valid_full['time_s'], df_valid[column_of_interest].values,\\\n",
    "                   lw=1, c='C1', label='valid')\n",
    "        ax.plot(df_test_full['time_s'],  df_test[column_of_interest].values,\\\n",
    "                   lw=1, c='C3', label='test')\n",
    "        ax.set_xticks([]); ax.set_yticks([]);\n",
    "\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), markerscale=5.0)\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary:\n",
    "        table = PrettyTable()\n",
    "        table.title = str(\"Fold's dataset shapes\")\n",
    "        table.field_names = ['set', 'shape']\n",
    "        table.add_row(['Train', np.shape(df_train)])\n",
    "        table.add_row(['Valid', np.shape(df_valid)])\n",
    "        table.add_row(['Test',  np.shape(df_test) ])\n",
    "        print(table)        \n",
    "        #---------------------------------------------------------------------\n",
    "\n",
    "        # Stationarying via L-S ----------------------------------------------\n",
    "        df_train_stat = df_train.copy(deep=True)\n",
    "        df_valid_stat = df_valid.copy(deep=True)\n",
    "        df_test_stat  = df_test.copy(deep=True)\n",
    "\n",
    "        if N_freqs == 'auto' or N_freqs > 0:\n",
    "\n",
    "            ls = LS(nterms=nterms)\n",
    "            ls.fit(df_train_full, column_time='time_s',\\\n",
    "                   column_signal=column_of_interest)\n",
    "\n",
    "            df_train_full_stat = ls.transform(df_train_full, color='C0')\n",
    "            df_valid_full_stat = ls.transform(df_valid_full, color='C1')\n",
    "            df_test_full_stat  = ls.transform(df_test_full,  color='C3')\n",
    "            \n",
    "            df_train_stat[column_of_interest] = \\\n",
    "                                df_train_full_stat[column_of_interest].values\n",
    "            df_valid_stat[column_of_interest] = \\\n",
    "                                df_valid_full_stat[column_of_interest].values\n",
    "            df_test_stat[column_of_interest]  = \\\n",
    "                                df_test_full_stat[column_of_interest].values\n",
    "            \n",
    "            # Summary:\n",
    "            table = PrettyTable()\n",
    "            table.title = str(\"Stationary dataset shapes\")\n",
    "            table.field_names = ['set', 'shape']\n",
    "            table.add_row(['Train', np.shape(df_train_stat)])\n",
    "            table.add_row(['Valid', np.shape(df_valid_stat)])\n",
    "            table.add_row(['Test',  np.shape(df_test_stat) ])\n",
    "            print(table) \n",
    "        #---------------------------------------------------------------------\n",
    "\n",
    "        # Windowing ----------------------------------------------------------\n",
    "        '''NOTE: The target column is placed last, by construction.'''\n",
    "        \n",
    "        print('Windowing train set')\n",
    "        X_train, X_idxs_train, y_train, y_idxs_train = \\\n",
    "            windowing_multi(df_train_stat,\n",
    "                win_size_predic=win_size, win_size_target=n_targets,\n",
    "                columns_target=[column_of_interest],\n",
    "                columns_predic=columns_predictors,\n",
    "                stride=stride, verbose=0)\n",
    "\n",
    "        print('Windowing validation set')\n",
    "        X_valid, X_idxs_valid, y_valid, y_idxs_valid = \\\n",
    "            windowing_multi(df_valid_stat,\n",
    "                win_size_predic=win_size, win_size_target=n_targets,\n",
    "                columns_target=[column_of_interest],\n",
    "                columns_predic=columns_predictors,\n",
    "                stride=stride, verbose=0)\n",
    "        \n",
    "        print('Windowing test set')\n",
    "        X_test, X_idxs_test, y_test, y_idxs_test = \\\n",
    "            windowing_multi(df_test_stat,\n",
    "                win_size_predic=win_size, win_size_target=n_targets,\n",
    "                columns_target=[column_of_interest],\n",
    "                columns_predic=columns_predictors,                            \n",
    "                stride=stride, verbose=0)\n",
    "        \n",
    "        n_features = X_train.shape[-1]\n",
    "\n",
    "        # Windowing L-S model:\n",
    "        '''NOTE: We need to window the L-S model as well, in the exact same\n",
    "                 way, to restore original signal'''\n",
    "        if N_freqs == 'auto' or N_freqs > 0:\n",
    "            \n",
    "            print('Windowing L-S model for train set')\n",
    "            df_model_LS_train = df_train_stat.copy(deep=True)\n",
    "            df_model_LS_train[column_of_interest] = \\\n",
    "                            ls.construct_model(df_train_full['time_s'].values)\n",
    "            \n",
    "            _, _, model_LS_y_train, _ = \\\n",
    "                windowing_multi(df_model_LS_train, win_size,\n",
    "                                win_size_target=n_targets, stride=stride,\n",
    "                                columns_target=[column_of_interest], verbose=0)\n",
    "            \n",
    "            print('Windowing L-S model for validation set')\n",
    "            df_model_LS_valid = df_valid_stat.copy(deep=True)\n",
    "            df_model_LS_valid[column_of_interest] = \\\n",
    "                            ls.construct_model(df_valid_full['time_s'].values)\n",
    "            \n",
    "            _, _, model_LS_y_valid, _ = \\\n",
    "                windowing_multi(df_model_LS_valid, win_size,\n",
    "                                win_size_target=n_targets, stride=stride,\n",
    "                                columns_target=[column_of_interest], verbose=0)\n",
    "\n",
    "            print('Windowing L-S model for test set')\n",
    "            df_model_LS_test = df_test_stat.copy(deep=True)\n",
    "            df_model_LS_test[column_of_interest] = \\\n",
    "                            ls.construct_model(df_test_full['time_s'].values)\n",
    "            \n",
    "            _, _, model_LS_y_test, _ = \\\n",
    "                windowing_multi(df_model_LS_test, win_size,\n",
    "                                win_size_target=n_targets, stride=stride,\n",
    "                                columns_target=[column_of_interest], verbose=0)\n",
    "\n",
    "        # Displaying window size:\n",
    "        fig, ax = plt.subplots(figsize=(12, 3), nrows=1, ncols=1,\n",
    "                                 gridspec_kw={'hspace': 0.5})\n",
    "\n",
    "        ax.set_title('Windowing: zoom-in on validation fold')\n",
    "        ax.scatter(df_valid_full['time_s'], df_valid_stat[column_of_interest],\\\n",
    "                   s=1, c='C1', label='valid')\n",
    "        idx_window = 0 # arbitrary index of a window\n",
    "        ax.plot(df_valid_full['time_s'][y_idxs_valid[idx_window]], y_valid[idx_window], \n",
    "                 label=str('%s-th window' % idx_window), c='darkorange', lw=2)\n",
    "        ax.axvspan(min(df_valid_full['time_s'][y_idxs_valid[idx_window]]),\n",
    "                   max(df_valid_full['time_s'][y_idxs_valid[idx_window]]),\n",
    "                   color='lightgray', alpha=0.8, label='Size of a window')\n",
    "\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), markerscale=5.0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # > Plotting a few windows\n",
    "        plot_a_few_windows(X_idxs_train, X_train, y_idxs_train, y_train, df_,\n",
    "                          title='Windowed data [training]') \n",
    "        \n",
    "        # Summary:\n",
    "        table = PrettyTable()\n",
    "        table.title = str('Windowed dataset shapes')\n",
    "        table.field_names = ['set', 'X shape', 'y shape']\n",
    "        table.add_row(['Train', np.shape(X_train), np.shape(y_train)])\n",
    "        table.add_row(['Valid', np.shape(X_valid), np.shape(y_valid)])\n",
    "        table.add_row(['Test',  np.shape(X_test),  np.shape(y_test)])\n",
    "        print(table)\n",
    "        #---------------------------------------------------------------------\n",
    "\n",
    "        # Smoothing ----------------------------------------------------------\n",
    "        '''Smoothing with a rolling window, but the smoothed version will only\n",
    "        be applied to the `X`s, not the `y`s.'''\n",
    "                \n",
    "        def replace_with_mavg(X, win_smooth=25):\n",
    "            '''Replace a column with its running average.'''\n",
    "            X_sm = np.zeros_like(X)\n",
    "\n",
    "            for j in range(np.shape(X)[-1]):\n",
    "                # iterating over dimensions (pandas DataFrames can only be 2D)\n",
    "                df = pd.DataFrame(X[:,:,j])\n",
    "                # Smooth:\n",
    "                df_out = df.rolling(window=win_smooth, center=True, axis=1, min_periods=1).mean()\n",
    "                # Replace `NaN`s with the corresponding original value:\n",
    "                df_out = df_out.fillna(df)\n",
    "                X_sm[:,:,j] = df_out.values\n",
    "                \n",
    "            return X_sm\n",
    "        \n",
    "        X_train_sm = replace_with_mavg(X_train, win_smooth=win_smooth)\n",
    "        X_valid_sm = replace_with_mavg(X_valid, win_smooth=win_smooth)\n",
    "        X_test_sm  = replace_with_mavg(X_test,  win_smooth=win_smooth)\n",
    "            \n",
    "        # Flattened arrays (for plot and scaling):\n",
    "        X_train_flat = invert_windowing(X_train, stride)\n",
    "        X_valid_flat = invert_windowing(X_valid, stride)\n",
    "        X_test_flat  = invert_windowing(X_test,  stride)\n",
    "        #\n",
    "        X_train_sm_flat = invert_windowing(X_train_sm, stride)\n",
    "        X_valid_sm_flat = invert_windowing(X_valid_sm, stride)\n",
    "        X_test_sm_flat  = invert_windowing(X_test_sm,  stride)\n",
    "        #\n",
    "        X_idxs_train_flat = invert_windowing(X_idxs_train, stride)\n",
    "        X_idxs_valid_flat = invert_windowing(X_idxs_valid, stride)\n",
    "        X_idxs_test_flat  = invert_windowing(X_idxs_test,  stride)\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=X_train_flat.shape[1], ncols=1,\\\n",
    "                                 figsize=(12, int(X_train_flat.shape[1]*3)))\n",
    "        if len(fig.axes) == 1: axes=[axes]\n",
    "        plt.suptitle('Original and smoothed predicting variables')\n",
    "        \n",
    "        # Iterating over features:\n",
    "        for j in range(X_train_flat.shape[1]):\n",
    "            # > Train\n",
    "            # Identify gaps in the training set:\n",
    "            gap_idxs = np.where(np.abs(np.diff(X_idxs_train_flat)) > 1)[0]\n",
    "\n",
    "            # Split training data at gaps and plot:\n",
    "            start = 0\n",
    "            for gap_idx in gap_idxs:\n",
    "                stop  = X_idxs_train_flat[gap_idx+1] - 1\n",
    "                axes[j].plot(X_idxs_train_flat[start:gap_idx+1],\n",
    "                             X_train_flat[start:gap_idx+1, j],\\\n",
    "                             c='C0', alpha=0.3)\n",
    "                axes[j].plot(X_idxs_train_flat[start:gap_idx+1],\n",
    "                             X_train_sm_flat[start:gap_idx+1, j],\\\n",
    "                             c='C0')\n",
    "                start = gap_idx + 1\n",
    "\n",
    "            axes[j].plot(X_idxs_train_flat[start:], X_train_flat[start:, j],\n",
    "                     c='C0', alpha=0.3, label='Orig. train')\n",
    "            axes[j].plot(X_idxs_train_flat[start:], X_train_sm_flat[start:, j],\n",
    "                     c='C0', label='Smooth train')\n",
    "                    \n",
    "            # > Valid\n",
    "            axes[j].plot(X_idxs_valid_flat, X_valid_flat[:, j],\n",
    "                     c='C1', alpha=0.3, label='Orig. valid')\n",
    "            axes[j].plot(X_idxs_valid_flat, X_valid_sm_flat[:, j],\n",
    "                     c='C1', label='Smooth valid')\n",
    "        \n",
    "            # > Test\n",
    "            axes[j].plot(X_idxs_test_flat, X_test_flat[:, j],\n",
    "                     c='C3', alpha=0.3, label='Orig. test')\n",
    "            axes[j].plot(X_idxs_test_flat, X_test_sm_flat[:, j],\n",
    "                     c='C3', label='Smooth test')\n",
    "\n",
    "            axes[j].set_ylabel(df_.columns[j])\n",
    "            axes[j].legend(loc='center left', bbox_to_anchor=(1, 0.5), markerscale=5.0)\n",
    "        \n",
    "        axes[-1].set_xlabel('index')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        #---------------------------------------------------------------------\n",
    "        \n",
    "        # Scaling ------------------------------------------------------------\n",
    "        #scaler_x = QuantileTransformer(n_quantiles=1000)\n",
    "        scaler_x = StandardScaler()\n",
    "        scaler_x.fit(X_train_sm_flat)\n",
    "        '''NOTE: The scaler must be trained on the data without repetitions,\n",
    "         or else the estimation of the standard deviaation will be biased.'''\n",
    "        X_train_n = scaler_x.transform(X_train_sm.reshape(\\\n",
    "            X_train_sm.shape[0]*X_train_sm.shape[1], -1)).reshape(X_train_sm.shape)\n",
    "        X_valid_n = scaler_x.transform(X_valid_sm.reshape(\\\n",
    "            X_valid_sm.shape[0]*X_valid_sm.shape[1], -1)).reshape(X_valid_sm.shape)\n",
    "        X_test_n  = scaler_x.transform(X_test_sm.reshape(\\\n",
    "             X_test_sm.shape[0]*X_test_sm.shape[1], -1)).reshape(X_test_sm.shape)\n",
    "\n",
    "        #scaler_y = QuantileTransformer(n_quantiles=1000)\n",
    "        scaler_y = StandardScaler()\n",
    "        y_train_n = scaler_y.fit_transform(y_train.reshape(-1, 1)).reshape(y_train.shape)\n",
    "        y_valid_n  = scaler_y.transform(y_valid.reshape(-1, 1)).reshape(y_valid.shape)\n",
    "        y_test_n   = scaler_y.transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "\n",
    "        # > Plotting\n",
    "        plot_a_few_windows(X_idxs_train, X_train_n, y_idxs_train, y_train_n, df_,\n",
    "                          title='Scaled data [training]') \n",
    "        #---------------------------------------------------------------------        \n",
    "        \n",
    "        # Imputing missing values --------------------------------------------\n",
    "        imp_x = SimpleImputer(strategy='constant', fill_value=np.nanmin(X_train_n))\n",
    "        imp_y = SimpleImputer(strategy='constant', fill_value=np.nanmin(X_train_n))\n",
    "\n",
    "        imp_x.fit( X_train_n.reshape(X_train_n.shape[0], -1) )\n",
    "        imp_y.fit( y_train_n )\n",
    "\n",
    "        def impute(X, imp):\n",
    "            X_filled = imp.transform(X.reshape(X.shape[0], -1)).reshape(X.shape)\n",
    "            return X_filled\n",
    "\n",
    "        X_train_n_f = impute(X_train_n, imp_x)\n",
    "        X_valid_n_f = impute(X_valid_n, imp_x)\n",
    "        X_test_n_f  = impute(X_test_n,  imp_x)\n",
    "        \n",
    "        y_train_n_f = imp_y.transform(y_train_n)\n",
    "        y_valid_n_f = imp_y.transform(y_valid_n)\n",
    "        y_test_n_f  = imp_y.transform(y_test_n)\n",
    "        \n",
    "        # > Creating masks ['False' where are NaNs, 'True' elsewhere]\n",
    "        mask_X_train = ~np.isnan(X_train_n)\n",
    "        mask_y_train = ~np.isnan(y_train_n)\n",
    "\n",
    "        # > Plotting\n",
    "        plot_a_few_windows(X_idxs_train, X_train_n_f, y_idxs_train, y_train_n_f, df_,\n",
    "                          title='Imputed data [training]') \n",
    "        #---------------------------------------------------------------------        \n",
    "        \n",
    "        # Fitting ------------------------------------------------------------\n",
    "\n",
    "        # Defining estimator by its hyperparameters:\n",
    "        if regr == 'LR':\n",
    "            subestimator = LinearRegression(**regr_params)\n",
    "\n",
    "        if regr == 'MLP':\n",
    "            subestimator = MLPRegressor(**regr_params, verbose=True, random_state=12)\n",
    "            # NOTE: Setting `random_state` as the MLP is very sensitive to it\n",
    "\n",
    "        if regr == 'LGB':\n",
    "            from sklearn.multioutput import MultiOutputRegressor\n",
    "            '''NOTE: LGB has no native support for multi-output regression.'''\n",
    "\n",
    "            regr_params['n_estimators'] = int(regr_params['n_estimators'])\n",
    "            regr_params['num_leaves']   = int(regr_params['num_leaves'])\n",
    "            regr_params['max_depth']    = int(regr_params['max_depth'])\n",
    "            # NOTE: Casting to `int` is necessary because they are\n",
    "            #       stored as `float` (since they can be a `NaN`)\n",
    "            \n",
    "            subestimator = MultiOutputRegressor(\n",
    "                               LGBMRegressor(**regr_params,\n",
    "                                   min_child_samples=12,#**\n",
    "                                   min_child_weight=1e-2,#**\n",
    "                                   max_bin=1024,#**\n",
    "                                   objective='regression',\n",
    "                                   verbosity=-1, seed=12)\n",
    "                            )\n",
    "            #**: Specified to try to avoid warning --- see:\n",
    "            #        https://stackoverflow.com/questions/47770123/lightgbm-how-to-deal-with-no-further-splits-with-positive-gain-best-gain-inf\n",
    "            \n",
    "        if regr == 'MCCNN':\n",
    "            \n",
    "            subestimator = build_MCCNN_model(\n",
    "                win_size=win_size, n_targets=n_targets, n_features=n_features,\n",
    "                **regr_params)\n",
    "\n",
    "        if regr == 'DLSTM':\n",
    "            \n",
    "            subestimator = build_DLSTM_model(\n",
    "                win_size=win_size, n_targets=n_targets, n_features=n_features,\n",
    "                **regr_params)\n",
    "\n",
    "        if regr == 'MHCNN':\n",
    "            \n",
    "            subestimator = build_MHCNN_model(\n",
    "                win_size=win_size, n_targets=n_targets, n_features=n_features,\n",
    "                **regr_params)\n",
    "\n",
    "        if regr == 'ED_LSTM':\n",
    "\n",
    "            subestimator = ED_LSTM_model(\n",
    "                win_size=win_size, n_targets=n_targets, n_features=n_features,\n",
    "                **regr_params)\n",
    "\n",
    "        if regr == 'LSTM':\n",
    "            \n",
    "            subestimator = build_LSTM_model(\n",
    "                win_size=win_size, n_targets=n_targets, n_features=n_features,\n",
    "                **regr_params)\n",
    "\n",
    "        if regr == 'WaveNet':\n",
    "            \n",
    "            subestimator = build_WaveNet_model(\n",
    "                win_size=win_size, n_targets=n_targets, n_features=n_features,\n",
    "                **regr_params)\n",
    "\n",
    "        if regr in ['WaveNet', 'MCCNN', 'DLSTM', 'MHCNN', 'LSTM', 'ED_LSTM']:\n",
    "            print(subestimator.summary())\n",
    "            \n",
    "            # Resetting model:\n",
    "            #subestimator.load_weights(path_results_main+'/'+'model_init_'+regr+'.h5')\n",
    "            '''\n",
    "            NOTE: The reset can be performed by reloading the initialization\n",
    "                  weights, as suggested here:\n",
    "                    https://stackoverflow.com/questions/40496069/reset-weights-in-keras-layer\n",
    "                  \n",
    "                  However, in this notebook, there will be a mismatch with the\n",
    "                  data shapes.\n",
    "                  Instead, the keras session is cleared inside the builders.\n",
    "                  \n",
    "            ''';\n",
    "\n",
    "        estimator = CustomEstimator(subestimator=subestimator)\n",
    "        display(estimator)                \n",
    "\n",
    "        # Fit the estimator on the training data:\n",
    "        if regr in ['WaveNet', 'MCCNN', 'DLSTM', 'MHCNN', 'LSTM']:\n",
    "            \n",
    "            early_stopping = EarlyStopping(monitor='val_loss', mode='min',\n",
    "               min_delta=0.0000005, patience=patience, verbose=1)\n",
    "            \n",
    "            estimator.fit(\n",
    "                X_train_n_f, y_train_n_f,\n",
    "                kwargs={\n",
    "                    'batch_size': int(regr_params['batch_size']),\n",
    "                    'epochs': 100,\n",
    "                    'shuffle': True,\n",
    "                    'callbacks': [time_callback, early_stopping],\n",
    "                    'validation_data':\\\n",
    "                        (X_valid_n_f, y_valid_n_f),   \n",
    "                }\n",
    "            )\n",
    "            # NOTE: Casting 'batch_size' to `int` is necessary\n",
    "            #       because they are stored as `float` (since they can be a `NaN`)\n",
    "            history = estimator.subestimator.history\n",
    "            \n",
    "        elif regr in ['ED_LSTM']:\n",
    "\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', mode='min',\n",
    "               min_delta=0.0000005, patience=patience, verbose=1)\n",
    "            \n",
    "            # Shifting timesteps to create additional decoder input:\n",
    "            def shift_y(y, filler=0):\n",
    "                '''Shifts the target matrix by one timestep backward, picking\n",
    "                the fist timestep from the previous window.'''\n",
    "\n",
    "                y_shift = np.empty_like(y)\n",
    "\n",
    "                # Shifting each row by one timestep\n",
    "                y_shift[0, 0] = filler\n",
    "                y_shift[0, 1:] = y[0, :-1]\n",
    "                for i in range(1, len(y)):\n",
    "                    y_shift[i, 0] = y[i-1, -1]\n",
    "                    y_shift[i, 1:] = y[i, :-1]\n",
    "                return y_shift\n",
    "            \n",
    "            y_train_n_f_shift = shift_y(y_train_n_f)\n",
    "            y_valid_n_f_shift = shift_y(y_valid_n_f)\n",
    "            \n",
    "            estimator.fit(\n",
    "                [X_train_n_f, y_train_n_f_shift], y_train_n_f,\n",
    "                kwargs={\n",
    "                    'batch_size': int(regr_params['batch_size']),\n",
    "                    'epochs': 100,\n",
    "                    'shuffle': True,\n",
    "                    'callbacks': [time_callback, early_stopping],\n",
    "                    'validation_data':\\\n",
    "                        ([X_valid_n_f, y_valid_n_f_shift], y_valid_n_f),\n",
    "                }\n",
    "            )\n",
    "                \n",
    "            # NOTE: Casting 'batch_size' to `int` is necessary\n",
    "            #       because they are stored as `float` (since they can be a `NaN`)\n",
    "            history = estimator.subestimator.history\n",
    "        \n",
    "        elif regr in ['LR', 'MLP']:\n",
    "\n",
    "            estimator.fit(\n",
    "                X_train_n_f.reshape(X_train_n_f.shape[0], -1), y_train_n_f\n",
    "            )\n",
    "        \n",
    "        elif regr == 'LGB':\n",
    "\n",
    "            estimator.fit(\n",
    "                X_train_n_f.reshape(X_train_n.shape[0], -1), y_train_n_f,\n",
    "                #kwargs={\n",
    "                #   'eval_set': [(X_valid_n_f.reshape(X_valid_n.shape[0], -1), y_valid_n_f)],\n",
    "                #   'early_stopping_rounds': 10\n",
    "                #}\n",
    "                # NOTE: Evaluation set cannot be used since the\n",
    "                #       MultiOutputRegressor cannot manage it.\n",
    "            )\n",
    "        \n",
    "        else:            \n",
    "            estimator.fit(\n",
    "                X_train_sm.reshape(X_train_n.shape[0], -1), y_train_n\n",
    "            )\n",
    "        #---------------------------------------------------------------------\n",
    "\n",
    "        # Plotting history (NNs only) ----------------------------------------\n",
    "        if regr in ['WaveNet', 'MCCNN', 'DLSTM', 'MHCNN', 'LSTM', 'ED_LSTM']:\n",
    "            fig, axes = plt.subplots(figsize=(12, 4), nrows=1, ncols=2)\n",
    "\n",
    "            axes[0].plot(history.history['MAE'], label='Training MAE',linewidth=4)\n",
    "            axes[0].plot(history.history['val_MAE'], label='Validation MAE',linewidth=4)\n",
    "\n",
    "            axes[1].plot(history.history['root_mean_squared_error'], label='Training RMSE',linewidth=4)\n",
    "            axes[1].plot(history.history['val_root_mean_squared_error'], label='Validation RMSE',linewidth=4)\n",
    "\n",
    "            for ax in axes:\n",
    "                ax.set_xlabel('Epoch')\n",
    "                ax.set_ylabel('Loss')\n",
    "                ax.set_yscale('log')\n",
    "                ax.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        #---------------------------------------------------------------------\n",
    "        \n",
    "        # Assessing ----------------------------------------------------------\n",
    "        scoring = {'MAE_train': [],  'MAE_valid': [],  'MAE_test': [],\n",
    "                   'RMSE_train': [], 'RMSE_valid': [], 'RMSE_test': []}\n",
    "\n",
    "        # Make predictions on the validation/test set:\n",
    "        if regr in ['WaveNet', 'MCCNN', 'DLSTM', 'MHCNN', 'LSTM', 'ED_LSTM'] and len(X_train_n.shape) == 3:        \n",
    "            yhat_train_stat_n_f = estimator.predict(X_train_n_f)\n",
    "            yhat_valid_stat_n_f = estimator.predict(X_valid_n_f)\n",
    "            yhat_test_stat_n_f  = estimator.predict(X_test_n_f)\n",
    "            \n",
    "            # Removing redundant features in case NN output is 1D:\n",
    "            yhat_train_stat_n_f = yhat_train_stat_n_f.squeeze()\n",
    "            yhat_valid_stat_n_f = yhat_valid_stat_n_f.squeeze()\n",
    "            yhat_test_stat_n_f  = yhat_test_stat_n_f.squeeze()\n",
    "            \n",
    "        else:\n",
    "            yhat_train_stat_n_f = estimator.predict(\n",
    "                            X_train_n_f.reshape(X_train_n_f.shape[0], -1))\n",
    "            yhat_valid_stat_n_f = estimator.predict(\n",
    "                            X_valid_n_f.reshape(X_valid_n_f.shape[0], -1))\n",
    "            yhat_test_stat_n_f  = estimator.predict(\n",
    "                            X_test_n_f.reshape(X_test_n_f.shape[0], -1))\n",
    "\n",
    "        # Inverting normalization:\n",
    "        yhat_train_stat_f = scaler_y.inverse_transform(yhat_train_stat_n_f)\n",
    "        yhat_valid_stat_f = scaler_y.inverse_transform(yhat_valid_stat_n_f)\n",
    "        yhat_test_stat_f  = scaler_y.inverse_transform(yhat_test_stat_n_f)\n",
    "\n",
    "        # De-stationarying (adding back windowed L-S model):\n",
    "        y_train_      = y_train\n",
    "        y_valid_      = y_valid\n",
    "        y_test_       = y_test\n",
    "        yhat_train_f  = yhat_train_stat_f\n",
    "        yhat_valid_f  = yhat_valid_stat_f\n",
    "        yhat_test_f   = yhat_test_stat_f\n",
    "        #\n",
    "        if N_freqs == 'auto' or N_freqs > 0:\n",
    "            y_train_      += model_LS_y_train\n",
    "            y_valid_      += model_LS_y_valid\n",
    "            y_test_       += model_LS_y_test\n",
    "            yhat_train_f  += model_LS_y_train\n",
    "            yhat_valid_f  += model_LS_y_valid\n",
    "            yhat_test_f   += model_LS_y_test\n",
    "\n",
    "        # Calculate scores for this fold:\n",
    "        '''NOTE: Excluding locations with NaNs in both 'true' and 'predicted'\n",
    "           (not to count in the metric)'''\n",
    "        \n",
    "        idxs_train_notnan = np.where(~np.isnan(y_train_))\n",
    "        idxs_valid_notnan = np.where(~np.isnan(y_valid_))\n",
    "        idxs_test_notnan  = np.where(~np.isnan(y_test_))\n",
    "\n",
    "        MAE_train  = mean_absolute_error(y_train_[idxs_train_notnan],\n",
    "                                     yhat_train_f[idxs_train_notnan])\n",
    "        MAE_valid  = mean_absolute_error(y_valid_[idxs_valid_notnan],\n",
    "                                     yhat_valid_f[idxs_valid_notnan])\n",
    "        MAE_test   = mean_absolute_error(y_test_[idxs_test_notnan],\n",
    "                                     yhat_test_f[idxs_test_notnan])\n",
    "        RMSE_train = mean_squared_error(y_train_[idxs_train_notnan],\n",
    "                            yhat_train_f[idxs_train_notnan], squared=False)\n",
    "        RMSE_valid = mean_squared_error(y_valid_[idxs_valid_notnan],\n",
    "                            yhat_valid_f[idxs_valid_notnan], squared=False)\n",
    "        RMSE_test  = mean_squared_error(y_test_[idxs_test_notnan],\n",
    "                            yhat_test_f[idxs_test_notnan], squared=False)\n",
    "\n",
    "        print('Train scores --- MAE: %.2f | RMSE: %.2f' % (MAE_train, RMSE_train))\n",
    "        print('Valid scores --- MAE: %.2f | RMSE: %.2f' % (MAE_valid, RMSE_valid))\n",
    "        print('Test  scores --- MAE: %.2f | RMSE: %.2f' % (MAE_test, RMSE_test))\n",
    "        \n",
    "        # Append scores to the scoring dictionary:\n",
    "        scoring['MAE_train'].append(MAE_train)\n",
    "        scoring['MAE_valid'].append(MAE_valid)\n",
    "        scoring['MAE_test'].append(MAE_test)\n",
    "        scoring['RMSE_train'].append(RMSE_train)    \n",
    "        scoring['RMSE_valid'].append(RMSE_valid)    \n",
    "        scoring['RMSE_test'].append(RMSE_test)    \n",
    "\n",
    "        df_results = pd.DataFrame(scoring)\n",
    "        \n",
    "        df_config = pd.DataFrame(config.items()).T\n",
    "        df_config.columns = df_config.iloc[0]\n",
    "        df_config = df_config[1:].reset_index(drop=True)\n",
    "        df_config.insert(0, 'model', label)\n",
    "\n",
    "        # Current scores:\n",
    "        df_scores_ = pd.concat([df_config, df_results], axis=1)\n",
    "        display(df_scores_)\n",
    "\n",
    "        # Appending:\n",
    "        df_scores = pd.concat([df_scores, df_scores_], axis=0, ignore_index=True)\n",
    "        #---------------------------------------------------------------------\n",
    "\n",
    "        # Plotting a few predictions -----------------------------------------\n",
    "        n_win_plot = 2\n",
    "        # number of consecutive windows to visualize\n",
    "        fig, axes = plt.subplots(figsize=(12, 2*n_win_plot), nrows=n_win_plot, ncols=3)\n",
    "\n",
    "        plt.suptitle('Predictions on some windows')\n",
    "        \n",
    "        for m, idx_m in zip([0, 1], [0,-1]):    \n",
    "        # windows along rows\n",
    "\n",
    "            # Train:\n",
    "            axes[m, 0].plot(y_idxs_train[idx_m], y_train[idx_m],      c='C0')\n",
    "            axes[m, 0].plot(y_idxs_train[idx_m], yhat_train_f[idx_m], c='C2')\n",
    "            # Valid:\n",
    "            axes[m, 1].plot(y_idxs_valid[idx_m], y_valid[idx_m],      c='C1')\n",
    "            axes[m, 1].plot(y_idxs_valid[idx_m], yhat_valid_f[idx_m], c='C2')\n",
    "            # Test:\n",
    "            axes[m, 2].plot(y_idxs_test[idx_m], y_test[idx_m],      c='C3')\n",
    "            axes[m, 2].plot(y_idxs_test[idx_m], yhat_test_f[idx_m], c='C2')\n",
    "\n",
    "            # Text:\n",
    "            def notnan_mae(y, yhat):\n",
    "                # Remove pairs where either element is NaN:\n",
    "                mask = ~np.isnan(y) & ~np.isnan(yhat)\n",
    "                y_notnan    = y[mask]\n",
    "                yhat_notnan = yhat[mask]\n",
    "                \n",
    "                mae = np.mean(np.abs(y_notnan - yhat_notnan))\n",
    "                return mae\n",
    "            \n",
    "            MAE_train_win = notnan_mae(y_train_[idx_m], yhat_train_f[idx_m])\n",
    "            MAE_valid_win = notnan_mae(y_valid_[idx_m], yhat_valid_f[idx_m])\n",
    "            MAE_test_win  = notnan_mae(y_test_[idx_m], yhat_test_f[idx_m])\n",
    "\n",
    "            text_kwargs = {'fontsize': 8, \n",
    "                'horizontalalignment': 'left', 'verticalalignment': 'center'}\n",
    "            \n",
    "            axes[m, 0].text(0.05, 0.18, str('MAE %.2f' % MAE_train_win),\n",
    "                            transform=axes[m, 0].transAxes, **text_kwargs)\n",
    "            axes[m, 1].text(0.05, 0.18, str('MAE %.2f' % MAE_valid_win),\n",
    "                            transform=axes[m, 1].transAxes, **text_kwargs)\n",
    "            axes[m, 2].text(0.05, 0.18, str('MAE %.2f' % MAE_test_win),\n",
    "                            transform=axes[m, 2].transAxes, **text_kwargs)\n",
    "            \n",
    "            for ax in [axes[m, 0], axes[m, 1], axes[m, 2]]:\n",
    "                ax.text(0.05, 0.1, str('window %s' % idx_m),\n",
    "                    fontsize=8, transform=ax.transAxes,\n",
    "                    horizontalalignment='left', verticalalignment='center')\n",
    "            \n",
    "            axes[m, 0].set_ylabel(column_of_interest)\n",
    "\n",
    "        axes[0, 0].set_title('Train', fontsize=10)\n",
    "        axes[0, 1].set_title('Valid', fontsize=10)\n",
    "        axes[0, 2].set_title('Test',  fontsize=10)\n",
    "\n",
    "        axes[n_win_plot-1, 0].set_xlabel('index')\n",
    "        axes[n_win_plot-1, 1].set_xlabel('index')\n",
    "        axes[n_win_plot-1, 2].set_xlabel('index')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.subplots_adjust(hspace=0.4)\n",
    "        plt.show()    \n",
    "        #---------------------------------------------------------------------\n",
    "\n",
    "        # Plotting mean predictions ------------------------------------------\n",
    "        # (i.e., mean across overlapping windows)\n",
    "\n",
    "        # Creating a \"matrix of predictions\":\n",
    "        '''\n",
    "        The matrix of predictions is a matrix of size:\n",
    "            n_windows * n_y\n",
    "        in which row \"i\" contains the `n_targets` predictions of window \"i\",\n",
    "        placed in the correct strided position along `y`, and `np.nan`\n",
    "        elsewhere.\n",
    "        E.g., for `n_targets` = 3 and `stride` = 1:\n",
    "            [0.64, 0.34, 0.54, NaN,  NaN,  NaN]\n",
    "            [NaN,  0.94, 0.72, 0.12, NaN,  NaN]\n",
    "            [NaN,  NaN,  0.71, 0.32, 0.84, NaN]\n",
    "            [...]\n",
    "        ''';\n",
    "        \n",
    "        # Number of total predicted timesteps:\n",
    "        n_unique_y_train = len(np.unique(y_idxs_train.flatten()))\n",
    "        n_unique_y_valid = len(np.unique(y_idxs_valid.flatten()))\n",
    "        n_unique_y_test  = len(np.unique(y_idxs_test.flatten()))\n",
    "        '''NOTE: This may be slightly different than `len(df_valid/test)`\n",
    "                 because some data-points at the end of the fold may be\n",
    "                 excluded by the windowing.'''\n",
    "        \n",
    "        def merge_predictions(yhat_set_f, n_unique_y_set, stride):\n",
    "            '''Convert overlapping predictions into a mean model +/- stdev.\n",
    "            \n",
    "            This function is meant to avoid RAM overload due to the large\n",
    "            matrices collecting all the predictions.\n",
    "            '''\n",
    "\n",
    "            M_yhat_set_f = np.full((len(yhat_set_f), n_unique_y_set), np.nan)\n",
    "            # initializing empty (`np.nan`) matrix of prediction\n",
    "\n",
    "            # Fill the matrix by striding and replacing `np.nan` where appropriate:        \n",
    "            for r, row in enumerate(yhat_set_f):\n",
    "                M_yhat_set_f[r, r*stride:r*stride+n_targets] = row.copy()\n",
    "            # replacing appropriate entries in matrix with strided values\n",
    "\n",
    "            with pd.option_context('display.max_rows', 4):\n",
    "                display(pd.DataFrame(M_yhat_set_f))\n",
    "            \n",
    "            # This step is extremely RAM-intensive:\n",
    "            mu_yhat_set_f  = np.nanmean(M_yhat_set_f, axis=0)\n",
    "            std_yhat_set_f = np.nanstd(M_yhat_set_f,  axis=0)\n",
    "            min_yhat_set_f = np.nanmin(M_yhat_set_f,  axis=0)\n",
    "            max_yhat_set_f = np.nanmax(M_yhat_set_f,  axis=0)\n",
    "\n",
    "            del M_yhat_set_f\n",
    "            return mu_yhat_set_f, std_yhat_set_f, min_yhat_set_f, max_yhat_set_f\n",
    "        \n",
    "        if debug:\n",
    "        # too RAM-intensive to be calculated for the training set\n",
    "            mu_yhat_train_f, std_yhat_train_f, min_yhat_train_f, max_yhat_train_f =\\\n",
    "                    merge_predictions(yhat_train_f, n_unique_y_train, stride)\n",
    "        mu_yhat_valid_f, std_yhat_valid_f, min_yhat_valid_f, max_yhat_valid_f =\\\n",
    "                merge_predictions(yhat_valid_f, n_unique_y_valid, stride)\n",
    "        mu_yhat_test_f, std_yhat_test_f, min_yhat_test_f, max_yhat_test_f =\\\n",
    "                merge_predictions(yhat_test_f, n_unique_y_test, stride)\n",
    "        \n",
    "        if debug:\n",
    "            y_train_flat_ = df_train_full.loc[\\\n",
    "                  np.unique(y_idxs_train.flatten()), column_of_interest].values\n",
    "        y_valid_flat_ = df_valid_full.loc[\\\n",
    "              np.unique(y_idxs_valid.flatten()), column_of_interest].values\n",
    "        y_test_flat_  = df_test_full.loc[\\\n",
    "              np.unique(y_idxs_test.flatten()), column_of_interest].values\n",
    "        \n",
    "        if debug:\n",
    "            fig, axes = plt.subplots(figsize=(12, 9), nrows=3, ncols=1)\n",
    "            ax_train = axes[0]\n",
    "            ax_valid = axes[1]\n",
    "            ax_test  = axes[2]\n",
    "        else:\n",
    "            fig, axes = plt.subplots(figsize=(12, 6), nrows=2, ncols=1)\n",
    "            ax_valid = axes[0]\n",
    "            ax_test  = axes[1]\n",
    "\n",
    "        plt.suptitle('Predictions averaged across overlapping windows',\n",
    "                     fontsize=12)        \n",
    "        \n",
    "        # Train: \n",
    "        if debug:\n",
    "            ax_train.plot(np.arange(0, len(mu_yhat_train_f), 1),\n",
    "                         mu_yhat_train_f, c='C2', label='mean')\n",
    "            ax_train.fill_between(np.arange(0, len(mu_yhat_train_f), 1),\n",
    "                                 mu_yhat_train_f, mu_yhat_train_f + std_yhat_train_f,\n",
    "                                 color='C2', alpha=0.3, label='1-$\\sigma$')\n",
    "            ax_train.fill_between(np.arange(0, len(mu_yhat_train_f), 1),\n",
    "                                 mu_yhat_train_f - std_yhat_train_f, mu_yhat_train_f,\n",
    "                                 color='C2', alpha=0.3)\n",
    "            ax_train.fill_between(np.arange(0, len(mu_yhat_train_f), 1),\n",
    "                                 mu_yhat_train_f, max_yhat_train_f,\n",
    "                                 color='C2', alpha=0.2, label='min/max')        \n",
    "            ax_train.fill_between(np.arange(0, len(mu_yhat_train_f), 1),\n",
    "                                 mu_yhat_train_f, min_yhat_train_f,\n",
    "                                 color='C2', alpha=0.2)        \n",
    "            \n",
    "            ax_train.plot(np.arange(0, len(mu_yhat_train_f), 1),\n",
    "                         y_train_flat_, c='C0', label='train')\n",
    "            ax_train.set_xlabel('time')\n",
    "            ax_train.set_ylabel('signal')\n",
    "            ax_train.legend(fontsize=10)\n",
    "        \n",
    "        # Validation: \n",
    "        ax_valid.plot(np.arange(0, len(mu_yhat_valid_f), 1),\n",
    "                     mu_yhat_valid_f, c='C2', label='mean')\n",
    "        ax_valid.fill_between(np.arange(0, len(mu_yhat_valid_f), 1),\n",
    "                             mu_yhat_valid_f, mu_yhat_valid_f + std_yhat_valid_f,\n",
    "                             color='C2', alpha=0.3, label='1-$\\sigma$')\n",
    "        ax_valid.fill_between(np.arange(0, len(mu_yhat_valid_f), 1),\n",
    "                             mu_yhat_valid_f - std_yhat_valid_f, mu_yhat_valid_f,\n",
    "                             color='C2', alpha=0.3)\n",
    "        ax_valid.fill_between(np.arange(0, len(mu_yhat_valid_f), 1),\n",
    "                             mu_yhat_valid_f, max_yhat_valid_f,\n",
    "                             color='C2', alpha=0.2, label='min/max')        \n",
    "        ax_valid.fill_between(np.arange(0, len(mu_yhat_valid_f), 1),\n",
    "                             mu_yhat_valid_f, min_yhat_valid_f,\n",
    "                             color='C2', alpha=0.2)        \n",
    "        ax_valid.plot(np.arange(0, len(mu_yhat_valid_f), 1),\n",
    "                     y_valid_flat_, c='C1', label='validation')\n",
    "        ax_valid.set_xlabel('time')\n",
    "        ax_valid.set_ylabel('signal')\n",
    "        ax_valid.legend(fontsize=10)\n",
    "        \n",
    "        # Test:\n",
    "        ax_test.plot(np.arange(0, len(mu_yhat_test_f), 1),\n",
    "                     mu_yhat_test_f, c='C2', label='mean')\n",
    "        ax_test.fill_between(np.arange(0, len(mu_yhat_test_f), 1),\n",
    "                             mu_yhat_test_f, mu_yhat_test_f + std_yhat_test_f,\n",
    "                             color='C2', alpha=0.3, label='1-$\\sigma$')\n",
    "        ax_test.fill_between(np.arange(0, len(mu_yhat_test_f), 1),\n",
    "                             mu_yhat_test_f - std_yhat_test_f, mu_yhat_test_f,\n",
    "                             color='C2', alpha=0.3)\n",
    "        ax_test.fill_between(np.arange(0, len(mu_yhat_test_f), 1),\n",
    "                             mu_yhat_test_f, max_yhat_test_f,\n",
    "                             color='C2', alpha=0.2, label='min/max')        \n",
    "        ax_test.fill_between(np.arange(0, len(mu_yhat_test_f), 1),\n",
    "                             mu_yhat_test_f, min_yhat_test_f,\n",
    "                             color='C2', alpha=0.2)                \n",
    "        ax_test.plot(np.arange(0, len(mu_yhat_test_f), 1),\n",
    "                     y_test_flat_, c='C3', label='test')\n",
    "        \n",
    "        ax_test.set_xlabel('time')\n",
    "        ax_test.set_ylabel('signal')\n",
    "        ax_test.legend(fontsize=10)    \n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        #---------------------------------------------------------------------\n",
    "        \n",
    "        # Storing temporary results ------------------------------------------\n",
    "        df_scores.to_csv(path_results_main+'/scores_tmp.csv')\n",
    "        #---------------------------------------------------------------------\n",
    "        \n",
    "        print('\\n\\n')\n",
    "\n",
    "    ##########################################################################\n",
    "    end_time = time.time()\n",
    "    config_time = end_time - start_time\n",
    "    if config_time >= 60:\n",
    "        minutes, seconds = divmod(config_time, 60)\n",
    "        print('%s time: %s min %s sec' % (label, int(minutes), int(seconds)))\n",
    "    else:\n",
    "        print('%s time: %.2f sec' % (label, config_time))\n",
    "\n",
    "display(df_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3351b",
   "metadata": {},
   "source": [
    "# Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca33068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 9), gridspec_kw={'hspace': 0.5})\n",
    "\n",
    "medianprops = {'linestyle': '-.',\n",
    "               'linewidth': 0.01, 'color': 'black'} \n",
    "# NOTE: Setting minimal linewidth will hide the median\n",
    "meanprops={\"markeredgecolor\": \"black\", \"markerfacecolor\": \"white\", \"markersize\": \"10\"}\n",
    "\n",
    "sns.boxplot(ax=axes[0], data=df_scores, x=\"RMSE_test\", y=\"model\",\n",
    "            medianprops=medianprops, showmeans=True, meanprops=meanprops)\n",
    "axes[0].set_xlabel('RMSE')\n",
    "\n",
    "sns.boxplot(ax=axes[1], data=df_scores, x=\"MAE_test\", y=\"model\",\n",
    "            medianprops=medianprops, showmeans=True, meanprops=meanprops)\n",
    "axes[1].set_xlabel('MAE')\n",
    "\n",
    "# sns.boxplot(ax=axes[2], data=df_scores, x=\"test_neg_MAXD\", y=\"model\",\n",
    "#             medianprops=medianprops, showmeans=True, meanprops=meanprops)\n",
    "# axes[2].set_xlabel('MAXD')\n",
    "\n",
    "for ax in axes:\n",
    "    sns.despine(ax=ax, offset=10)\n",
    "    \n",
    "plt.savefig(path_results_main+'/CV.png')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b8f2d",
   "metadata": {},
   "source": [
    "# Exporting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3dbcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv(path_results_main+'/scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a13689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf4812",
   "metadata": {},
   "outputs": [],
   "source": [
    "###EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ca756",
   "metadata": {},
   "outputs": [],
   "source": [
    "###EOF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn",
   "language": "python",
   "name": "rnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "327.99px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.589px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
